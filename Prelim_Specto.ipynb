{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajD1VOxQU6E3"
   },
   "source": [
    "# Cereal Time Killers: Deep learning model to predict emotional states based on EEG data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BH63Pq7kVZ3N"
   },
   "source": [
    "[TODO: Introduction and description of project]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrHDz70eVUFp"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9-rL45RVVdj"
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ie0GR2fLVF9Z"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import pathlib\n",
    "import scipy.signal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset,DataLoader, TensorDataset\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IwPY6cAXkqA"
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-Vg9erREX989"
   },
   "outputs": [],
   "source": [
    "def get_specgram(dir_to_GAMEEMO, patient, game,winlen=None,stride=1):\n",
    "    # Reading from the csv data set (can do matlab as well) using pandas. \n",
    "\n",
    "    #Patient = 'S01' #The Patient\n",
    "    #Game = 'G1' #The game\n",
    "\n",
    "    #You can also just paste in the Directory of the csv file - on windows you may have to change the slash direction\n",
    "    DirComb = f'{dir_to_GAMEEMO}/({patient})/Preprocessed EEG Data/.csv format/{patient}{game}AllChannels.csv'\n",
    "\n",
    "    df=pd.read_csv(DirComb, sep=',',header=None)\n",
    "    d = np.array(df) #Switching from pandas to numpy array as this might be more comfortable for people\n",
    "    d = np.delete(d,0,0) # Deleting header\n",
    "    d = np.delete(d,-1,1).astype(float) #and erroneous last column\n",
    "    \n",
    "    full_spec = []\n",
    "    for idx, d2 in enumerate(d.T):\n",
    "        _, _, Sxx = spectrogram(d2,fs=120)\n",
    "        full_spec.append(Sxx)\n",
    "        \n",
    "    #DIMENSIONS OF FULL_SPEC WITHOUT WINDOWING\n",
    "    #DIMENSION 1: CHANNELS  (DEFAULT=14) - MIGHT CHANGE (SO NOT REALLY DEFAULT BUT OK)\n",
    "    #DIMENSION 2: FREQUENCY (DEFAULT=129)\n",
    "    #DIMENSION 3: TIME      (DEFAULT=170) - MIGHT CHANGE AS WELL OK - WE ARE WORKING ON IT\n",
    "    \n",
    "    full_spec = np.vstack([full_spec])\n",
    "\n",
    "    if(winlen==None):\n",
    "        return full_spec\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    \n",
    "    full_spec_wind = []\n",
    "    while i*stride+winlen<full_spec.shape[-1]:\n",
    "        full_spec_wind.append(full_spec[:,:,stride*i:i*stride+winlen])\n",
    "        i+=1    \n",
    "    \n",
    "    #DIMENSIONS OF FULL_SPEC WITH WINDOWING    (FULL_SPEC_WIND) \n",
    "    #DIMENSION 1: CHANNELS  (DEFAULT=14) - MIGHT CHANGE (SO NOT REALLY DEFAULT BUT OK)\n",
    "    #DIMENSION 2: FREQUENCY (DEFAULT=129)\n",
    "    #DIMENSION 3: TIME      (NO DEFAULT - SORRY)\n",
    "    #DIMENSION 4: WINDOWS   (NO DEFAULT - SORRY)\n",
    "    \n",
    "    full_spec_wind = np.array(full_spec_wind)\n",
    "    full_spec_wind = np.moveaxis(full_spec_wind,0,-1)\n",
    "    return full_spec_wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/Users/maximilianeggl/Dropbox/PostDoc/NeuroMatch/'\n",
    "labeldir = f'{basedir}GameLabels.csv' #Path to GAMEEMO\n",
    "gamedir = f'{basedir}GAMEEMO/'\n",
    "\n",
    "full_spec = get_specgram(gamedir, 'S01', 'G1',25,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-166-ea7a13fb5f49>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels_df['full_specgram_1'][idx] = torch.tensor(get_specgram(gamedir, subject, game)).double()\n"
     ]
    }
   ],
   "source": [
    "labels_df = pd.read_csv(labeldir)\n",
    "\n",
    "labels_df['full_specgram_1'] = [[]] * len(labels_df)\n",
    "for idx in range(len(labels_df)):\n",
    "    game = f'G{int(idx % 4)+1}'\n",
    "    subject = f'S{int(idx/4)+1:02d}'\n",
    "    labels_df['full_specgram_1'][idx] = torch.tensor(get_specgram(gamedir, subject, game)).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CerealTimeKillersDataset(Dataset):\n",
    "    \"\"\"Spectrogram dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.ori_dataframe = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ori_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        spectrogram = self.ori_dataframe.iloc[idx, 14]\n",
    "        labels = self.ori_dataframe.iloc[idx, :14]\n",
    "        labels = torch.tensor([labels[8:]])\n",
    "        sample = (spectrogram,labels)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = CerealTimeKillersDataset(labels_df)\n",
    "train_loader,test_loader,val_loader = torch.utils.data.random_split(final_dataset, [40,40,28])\n",
    "train_loader = DataLoader(train_loader, batch_size=4, shuffle=True,\n",
    "                        num_workers=0)\n",
    "test_loader = DataLoader(test_loader, batch_size=4, shuffle=True,\n",
    "                        num_workers=0)\n",
    "val_loader = DataLoader(val_loader, batch_size=4, shuffle=True,\n",
    "                        num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 7, 5, 5, 4, 5]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYdQIKABXpzn"
   },
   "source": [
    "# Implementation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPcS5fDUXtEs"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLadbf4uXtny"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LHJX_gyYrwk"
   },
   "source": [
    "# Model: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "DoIbKh_XLXSi"
   },
   "outputs": [],
   "source": [
    "class CTK_Net(nn.Module):\n",
    "  def __init__(self, out_size, img_shape):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "      out_size : size of the output. It should match the number of labels in the dataset.\n",
    "      img_shape : list len 2\n",
    "    \"\"\"\n",
    "    super(CTK_Net, self).__init__()\n",
    "\n",
    "    self.conv1 = nn.Conv2d(in_channels=14, out_channels=32, kernel_size=3)\n",
    "    #self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "    # WARNING: change the hardcoded values in fc1_input_size if you change the architecture!!!\n",
    "    fc1_input_size = int(32 * np.prod((np.array(img_shape) - 3 + 1 - 3 + 1) / 2))\n",
    "    self.fc1 = nn.Linear(in_features=fc1_input_size, out_features=128)\n",
    "    self.drop1 = nn.Dropout(.5)\n",
    "    self.fc2 = nn.Linear(in_features=128, out_features=out_size)\n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = F.relu(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.fc1(x)\n",
    "    x = F.sigmoid(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'epochs': 150,\n",
    "    'lr': 5e-3,\n",
    "    'momentum': 0.99,\n",
    "    'device': 'cpu',\n",
    "}\n",
    "\n",
    "TestNet = CTK_Net(5,np.array((149,170)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "6hthtFM3V974"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-194-5c50dcf4bb46>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-194-5c50dcf4bb46>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    outputs = model(inp'uts)\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "def train(args, model, train_loader, optimizer=None, criterion=F.nll_loss):\n",
    "  model.train()\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss() #Change as desired\n",
    "  optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=args['lr'])\n",
    "  for epoch in range(args['epochs']):\n",
    "    with tqdm(train_loader, unit='batch') as tepoch:\n",
    "      for data, target in tepoch:\n",
    "        data, target = data.type(torch.float).to(device), target.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # if reg_function1 is None:\n",
    "        #   loss = criterion(output, target)\n",
    "        # elif reg_function2 is None:\n",
    "        #   loss = criterion(output, target)+args['lambda']*reg_function1(model)\n",
    "        # else:\n",
    "        #   loss = criterion(output, target) + args['lambda1']*reg_function1(model) + args['lambda2']*reg_function2(model)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tepoch.set_postfix(loss=loss.item())\n",
    "        time.sleep(0.1)\n",
    "\n",
    "\n",
    "def test(model, device, data_loader):\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for data in data_loader:\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.to(device).float()\n",
    "    labels = labels.to(device).long()\n",
    "\n",
    "    outputs = model(inp'uts)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "  acc = 100 * correct / total\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10467437efeb4f2f895a06808228114c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-192-73c42ae6b5f3>\u001b[0m(22)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     20 \u001b[0;31m  \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     21 \u001b[0;31m    \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 22 \u001b[0;31m    \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     23 \u001b[0;31m    \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     24 \u001b[0;31m    \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> x = x[0]\n",
      "ipdb> x.shape\n",
      "torch.Size([14, 129, 170])\n",
      "ipdb> n\n",
      "RuntimeError: Expected 4-dimensional input for 4-dimensional weight [32, 14, 3, 3], but got 3-dimensional input of size [14, 129, 170] instead\n",
      "> \u001b[0;32m<ipython-input-192-73c42ae6b5f3>\u001b[0m(22)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     20 \u001b[0;31m  \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     21 \u001b[0;31m    \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 22 \u001b[0;31m    \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     23 \u001b[0;31m    \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     24 \u001b[0;31m    \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train(args,TestNet,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CerealTimeKillers.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
