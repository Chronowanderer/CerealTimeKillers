{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "# ML packages\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, SubsetRandomSampler, random_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Setup package manually implemented\n",
    "from setup import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CerealTimeKillersDataset(Dataset):\n",
    "    \"\"\"Spectrogram dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, transform = None):\n",
    "        self.ori_dataframe = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ori_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        spectrogram = self.ori_dataframe.iloc[idx, -1]\n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        labels = self.ori_dataframe.iloc[idx, :-1]\n",
    "        labels = np.array([labels])\n",
    "#         if self.transform:\n",
    "#             labels = self.transform(labels)\n",
    "        \n",
    "        sample = {'spectrogram': spectrogram, 'labels': labels}\n",
    "        return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specgram(file_dir, labels, winlen = None, stride = 1, nperseg = 256, fs = 129):\n",
    "    \n",
    "    # Reading from the csv data set (can do matlab as well) using pandas. \n",
    "    df = pd.read_csv(file_dir, sep = ',')\n",
    "    df = pd.DataFrame(df, columns = labels)\n",
    "    d = np.array(df, dtype = float) # Switching from pandas to numpy array as this might be more comfortable for people\n",
    "    \n",
    "    full_spec = []\n",
    "    for idx, d2 in enumerate(d.T):\n",
    "        _, _, Sxx = spectrogram(d2, nperseg = nperseg, fs = fs)\n",
    "        full_spec.append(Sxx)\n",
    "        \n",
    "    #DIMENSIONS OF FULL_SPEC WITHOUT WINDOWING (I.E. FULL WINDOWING)\n",
    "    #DIMENSION 1: 1                      - FOR DIMENSIONAL CONSISTENCY\n",
    "    #DIMENSION 2: TIME      (DEFAULT=170) - MIGHT CHANGE AS WELL OK - WE ARE WORKING ON IT\n",
    "    #DIMENSION 3: CHANNELS  (DEFAULT=14) - MIGHT CHANGE (SO NOT REALLY DEFAULT BUT OK)\n",
    "    #DIMENSION 4: FREQUENCY (DEFAULT=129)\n",
    "    \n",
    "    full_spec = np.vstack([full_spec])\n",
    "    full_spec = np.moveaxis(full_spec, -1, 0)\n",
    "    if winlen == None:\n",
    "        return np.array([full_spec])\n",
    "    \n",
    "    i = 0\n",
    "    full_spec_wind = []\n",
    "    while i * stride + winlen < full_spec.shape[-1]:\n",
    "        full_spec_wind.append(full_spec[i * stride : i * stride + winlen, : , :])\n",
    "        i += 1\n",
    "    \n",
    "    #DIMENSIONS OF FULL_SPEC WITH WINDOWING    (FULL_SPEC_WIND) \n",
    "    #DIMENSION 1: TIME      (NO DEFAULT - SORRY)\n",
    "    #DIMENSION 2: WINDOWS   (DEFAULT=1)\n",
    "    #DIMENSION 3: CHANNELS  (DEFAULT=14) - MIGHT CHANGE (SO NOT REALLY DEFAULT BUT OK)\n",
    "    #DIMENSION 4: FREQUENCY (DEFAULT=129)\n",
    "    \n",
    "    full_spec_wind = np.array(full_spec_wind)\n",
    "    return full_spec_wind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CerealTimeKillersDataLoader(dir_class, label_class, dataset_mix = True, \n",
    "                                winlen = None, stride = 1, nperseg = 256, fs = 129,\n",
    "                                transform = None):\n",
    "    \n",
    "    specgram_name = 'full_specgram_1'\n",
    "    \n",
    "    # Load label & EEG data\n",
    "    labels_df = pd.read_csv(dir_class.label)\n",
    "    if dataset_mix:\n",
    "        spec_df = pd.DataFrame(columns = label_class.fixed + [specgram_name], dtype = float)\n",
    "        index_df = pd.DataFrame(columns = ['subject', 'game'], dtype = int)\n",
    "    else:\n",
    "        spec_df = pd.DataFrame(labels_df, columns = label_class.fixed + [specgram_name], dtype = float)\n",
    "        spec_df[specgram_name] = [[]] * len(spec_df)\n",
    "        index_df = pd.DataFrame(labels_df, columns = ['subject', 'game'], dtype = int)\n",
    "    \n",
    "    # Create spectrogram dataframe\n",
    "    for idx in range(labels_df.shape[0]): \n",
    "        subject = labels_df['subject'].iloc[idx]\n",
    "        game = labels_df['game'].iloc[idx]\n",
    "    \n",
    "        # You can also just paste in the Directory of the csv file - on windows you may have to change the slash direction\n",
    "        DirComb = f'{dir_class.game}/(S{str(subject).zfill(2)})/Preprocessed EEG Data/.csv format/S{str(subject).zfill(2)}G{str(game)}AllChannels.csv'\n",
    "        \n",
    "        # Get EEG spectrogram\n",
    "        spec_EEG = get_specgram(DirComb, label_class.electrode, \n",
    "                                winlen = winlen, stride = stride, nperseg = nperseg, fs = fs)\n",
    "        if dataset_mix:\n",
    "            # Add new data to dataframe\n",
    "            new_spec_list, new_index_list = list(), list()\n",
    "            for i in range(spec_EEG.shape[0]):\n",
    "                new_spec_list.append(list(labels_df[label_class.fixed].iloc[idx]) + [spec_EEG[i]])\n",
    "                new_index_list.append([subject, game])\n",
    "            new_spec_df = pd.DataFrame(new_spec_list, columns = label_class.fixed + [specgram_name], dtype = float)\n",
    "            spec_df = pd.concat([spec_df, new_spec_df], ignore_index = True)    \n",
    "            new_index_df = pd.DataFrame(new_index_list, columns = ['subject', 'game'], dtype = int)\n",
    "            index_df = pd.concat([index_df, new_index_df], ignore_index = True)\n",
    "        else:\n",
    "            spec_df[specgram_name].iloc[idx] = spec_EEG\n",
    "\n",
    "    return CerealTimeKillersDataset(df = spec_df, transform = transform), index_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CerealTimeKillersDataSplitter(full_dataset, exp_index, \n",
    "                                  allocation_test = None, \n",
    "                                  test_ratio = 0.2, target_test = [], k_folds = 10, \n",
    "                                  batch_size_train = 64, batch_size_test = 256, \n",
    "                                  seed = 0, generator = None):\n",
    "    \n",
    "    # Split into train/val and test datasets\n",
    "    train_set_index, test_set_index = list(), list()\n",
    "    if allocation_test == None:\n",
    "        test_size = int(test_ratio * len(full_dataset))\n",
    "        train_size = len(full_dataset) - test_size\n",
    "        train_set_orig, test_set_orig = random_split(full_dataset, \n",
    "                                                     [train_size, test_size], \n",
    "                                                     generator = generator)\n",
    "    elif (allocation_test == 'subject') or (allocation_test == 'game'):\n",
    "        train_set_index = exp_index[~exp_index[allocation_test].isin(target_test)].index.tolist()\n",
    "        test_set_index = exp_index[exp_index[allocation_test].isin(target_test)].index.tolist()\n",
    "        train_set_orig = Subset(full_dataset, train_set_index)\n",
    "        test_set_orig = Subset(full_dataset, test_set_index)\n",
    "    else:\n",
    "        print(\"Allocate testing dataset based on one of the 'Subject', 'Game', or None.\")\n",
    "        return None\n",
    "    \n",
    "    # Test dataset loader\n",
    "    test_loader = DataLoader(test_set_orig,\n",
    "                             batch_size = batch_size_test,\n",
    "                             num_workers = 2,\n",
    "                             generator = g_seed)\n",
    "    \n",
    "    # K-fold Cross Validator\n",
    "    train_loader, val_loader = [[]] * k_folds, [[]] * k_folds\n",
    "    kfold = KFold(n_splits = k_folds, shuffle = True, random_state = seed)\n",
    "    for fold, (train_i, val_i) in enumerate(kfold.split(train_set_orig)):\n",
    "        \n",
    "        # Sample train/test dataset from indices\n",
    "        train_sampler = SubsetRandomSampler(train_i, generator = g_seed)\n",
    "        val_sampler = SubsetRandomSampler(val_i, generator = g_seed)\n",
    "        \n",
    "        # Train/Validation dataset loader\n",
    "        train_loader[fold] = DataLoader(train_set_orig,\n",
    "                                        sampler = train_sampler,\n",
    "                                        batch_size = batch_size_train,\n",
    "                                        num_workers = 2,\n",
    "                                        generator = generator)\n",
    "        val_loader[fold] = DataLoader(train_set_orig,\n",
    "                                      sampler = val_sampler,\n",
    "                                      batch_size = batch_size_test,\n",
    "                                      num_workers = 2,\n",
    "                                      generator = generator)\n",
    "    \n",
    "    # return datasplitter\n",
    "    data_loader = {'train': train_loader, 'val': val_loader, 'test': test_loader}\n",
    "    return data_loader, (len(train_sampler), len(val_sampler), len(test_set_orig))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CerealTimeKillersLabels:\n",
    "    \"\"\"Select labels for model prediction.\"\"\"\n",
    "    # Labels used for prediction: Label_info + Label_electrode --> Label_prediction\n",
    "    # CHANGE these with necessity\n",
    "    \n",
    "    # ['subject', 'game', 'gender', 'age', 'disturbance', 'experience', 'memory']\n",
    "    info = []\n",
    "        \n",
    "    # ['AF3', 'AF4', 'F3', 'F4', 'F7', 'F8', 'FC5', 'FC6', 'O1', 'O2', 'P7', 'P8', 'T7', 'T8']\n",
    "    electrode = ['AF3', 'AF4', 'F3', 'F4', 'F7', 'F8', 'FC5', 'FC6', 'O1', 'O2', 'P7', 'P8', 'T7', 'T8']\n",
    "        \n",
    "    # ['satisfied', 'boring', 'horrible', 'calm', 'funny', 'valence', 'arounsal']\n",
    "    prediction = ['boring', 'horrible', 'calm', 'funny']\n",
    "    # prediction = ['valence', 'arounsal']\n",
    "    \n",
    "    # Fixed variables\n",
    "    fixed = info + prediction\n",
    "    \n",
    "    # Summarise labels for model\n",
    "    label = info + electrode + prediction\n",
    "\n",
    "    \n",
    "class CerealTimeKillersDir:\n",
    "    \"\"\"Directionary for folders.\"\"\"\n",
    "    base = ''\n",
    "    label = f'{base}GameLabels.csv'\n",
    "    game = f'{base}GAMEEMO'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2021 has been set.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd245fe33b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General settings\n",
    "# Whether to allow between-subject and between-game dataset mixture\n",
    "Is_between_subject = True # Default is True\n",
    "# Which to be based for allocating testing dataset (only when Is_between_subject = True)\n",
    "Allocation_test = None # [None, 'subject', 'game'] # Default is None\n",
    "test_ratio = 0.2 # Proportion of data used for testing when Allocation_test == None\n",
    "Target_test = [25, 26, 27] # Int list for allocating corresponding game/subject as testing dataset\n",
    "\n",
    "# Model structural settings\n",
    "N_inputtime = None # Time window for input sampling (Default is None for the whole timepoints)\n",
    "N_stridetime = 1 # Temporal leap for input sampling\n",
    "N_perseg = 256 # N per seg of spectrogram\n",
    "N_framerate = 128 # Framerate of spectrogram\n",
    "\n",
    "# Model training settings\n",
    "batch_size_train = 16 # Number of examples per minibatch during training\n",
    "batch_size_test = 32 # Number of examples per minibatch during validation/testing\n",
    "k_folds = 10 # Number for K-folds for training vs validation\n",
    "\n",
    "# Data transformation\n",
    "if Is_between_subject:\n",
    "    data_transform = transforms.Compose([transforms.ToTensor()])\n",
    "else:\n",
    "    data_transform = None\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 2021\n",
    "set_seed(seed = SEED)\n",
    "g_seed = torch.Generator()\n",
    "g_seed.manual_seed(SEED)\n",
    "\n",
    "# Set device\n",
    "# DEVICE = set_device()\n",
    "# print('Current device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Dataloader\n",
    "FullDataset, ExpIndex = CerealTimeKillersDataLoader(CerealTimeKillersDir,\n",
    "                                                    CerealTimeKillersLabels,\n",
    "                                                    dataset_mix = Is_between_subject,\n",
    "                                                    winlen = N_inputtime,\n",
    "                                                    stride = N_stridetime,\n",
    "                                                    nperseg = N_perseg,\n",
    "                                                    fs = N_framerate,\n",
    "                                                    transform = data_transform)\n",
    "\n",
    "# Implement DataSplitter\n",
    "SplittedDataset, SplittedDataLength = CerealTimeKillersDataSplitter(FullDataset, \n",
    "                                                                    exp_index = ExpIndex, \n",
    "                                                                    allocation_test = Allocation_test,\n",
    "                                                                    test_ratio = test_ratio,\n",
    "                                                                    target_test = Target_test,\n",
    "                                                                    k_folds = k_folds,\n",
    "                                                                    batch_size_train = batch_size_train,\n",
    "                                                                    batch_size_test = batch_size_test,\n",
    "                                                                    seed = SEED,\n",
    "                                                                    generator = g_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test Dataset length: (79, 8, 21)\n",
      "\n",
      "1/10 Fold\n",
      "----------------------------\n",
      "Input data size: torch.Size([129, 170, 14])\n",
      "Output data size: (1, 4)\n",
      "Output label example: [[7.0 1.0 8.0 3.0]]\n",
      "\n",
      "2/10 Fold\n",
      "----------------------------\n",
      "Input data size: torch.Size([129, 170, 14])\n",
      "Output data size: (1, 4)\n",
      "Output label example: [[7.0 1.0 8.0 3.0]]\n",
      "\n",
      "3/10 Fold\n",
      "----------------------------\n",
      "Input data size: torch.Size([129, 170, 14])\n",
      "Output data size: (1, 4)\n",
      "Output label example: [[7.0 1.0 8.0 3.0]]\n",
      "\n",
      "4/10 Fold\n",
      "----------------------------\n",
      "Input data size: torch.Size([129, 170, 14])\n",
      "Output data size: (1, 4)\n",
      "Output label example: [[7.0 1.0 8.0 3.0]]\n",
      "\n",
      "5/10 Fold\n",
      "----------------------------\n",
      "Input data size: torch.Size([129, 170, 14])\n",
      "Output data size: (1, 4)\n",
      "Output label example: [[7.0 1.0 8.0 3.0]]\n",
      "\n",
      "6/10 Fold\n",
      "----------------------------\n",
      "Input data size: torch.Size([129, 170, 14])\n",
      "Output data size: (1, 4)\n",
      "Output label example: [[7.0 1.0 8.0 3.0]]\n",
      "\n",
      "7/10 Fold\n",
      "----------------------------\n",
      "Input data size: torch.Size([129, 170, 14])\n",
      "Output data size: (1, 4)\n",
      "Output label example: [[7.0 1.0 8.0 3.0]]\n",
      "\n",
      "8/10 Fold\n",
      "----------------------------\n",
      "Input data size: torch.Size([129, 170, 14])\n",
      "Output data size: (1, 4)\n",
      "Output label example: [[7.0 1.0 8.0 3.0]]\n",
      "\n",
      "9/10 Fold\n",
      "----------------------------\n",
      "Input data size: torch.Size([129, 170, 14])\n",
      "Output data size: (1, 4)\n",
      "Output label example: [[7.0 1.0 8.0 3.0]]\n",
      "\n",
      "10/10 Fold\n",
      "----------------------------\n",
      "Input data size: torch.Size([129, 170, 14])\n",
      "Output data size: (1, 4)\n",
      "Output label example: [[7.0 1.0 8.0 3.0]]\n"
     ]
    }
   ],
   "source": [
    "(TrainDataLoader, ValDataLoader, TestDataLoader) = (SplittedDataset['train'],\n",
    "                                                    SplittedDataset['val'],\n",
    "                                                    SplittedDataset['test'])\n",
    "\n",
    "print('Train/Val/Test Dataset length:', SplittedDataLength)\n",
    "for fold in range(k_folds):\n",
    "    idx = 0\n",
    "    print('\\n%d/%d Fold' % (fold + 1, k_folds))\n",
    "    print('----------------------------')\n",
    "    print('Input data size:', TrainDataLoader[fold].dataset[idx]['spectrogram'].size())\n",
    "    print('Output data size:', TrainDataLoader[fold].dataset[idx]['labels'].shape)\n",
    "    print('Output label example:', TrainDataLoader[fold].dataset[idx]['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
