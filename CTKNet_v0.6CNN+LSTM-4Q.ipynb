{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CerealTimeKillersNet: Deep neural network for emotional states predictions from EEG data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Math packages\n",
    "import math\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "# Plot packages\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, SubsetRandomSampler, random_split\n",
    "\n",
    "# ML Packages\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functions setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "# Executing `set_seed(seed=seed)` you are setting the seed to ensure reproducibility.\n",
    "# for DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "def set_seed(seed = None, seed_torch = True):\n",
    "    if seed is None:\n",
    "        seed = np.random.choice(2 ** 32)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if seed_torch:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    print(f'Random seed {seed} has been set.')\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()` especially if torch modules used.\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"WARNING: For this notebook to perform best, \"\n",
    "            \"if possible, in the menu under `Runtime` -> \"\n",
    "            \"`Change runtime type.`  select `GPU` \")\n",
    "    else:\n",
    "        print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot settings\n",
    "\n",
    "def plot_loss_accuracy(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc,\n",
    "                       chance = 0.25):\n",
    "    epochs = len(train_loss)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.plot(list(range(epochs)), train_loss, label = 'Training')\n",
    "    ax1.plot(list(range(epochs)), val_loss, label = 'Validation')\n",
    "    ax1.plot(list(range(epochs)), test_loss, label = 'Testing')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Epoch vs Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(list(range(epochs)), train_acc, label = 'Training')\n",
    "    ax2.plot(list(range(epochs)), val_acc, label = 'Validation')\n",
    "    ax2.plot(list(range(epochs)), test_acc, label = 'Testing')\n",
    "    ax2.plot(list(range(epochs)), [chance * 100] * epochs, 'k--', label = 'Baseline')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_ylim([0, 100])\n",
    "    ax2.set_title('Epoch vs Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    fig.set_size_inches(15.5, 5.5)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norm\n",
    "def calculate_frobenius_norm(model):\n",
    "    norm = 0.0\n",
    "    for param in model.parameters():\n",
    "        norm += torch.sum(param ** 2)\n",
    "    norm = norm ** 0.5\n",
    "    return norm\n",
    "\n",
    "def L1_norm(model):\n",
    "    return sum(p.abs().sum() for p in model.parameters())\n",
    "\n",
    "def L2_norm(model):\n",
    "    return sum((p**2).sum() for p in model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum label comparison for accracy\n",
    "\n",
    "def maximum_extraction(tens, eplison = 1e-8):\n",
    "    l_index = []\n",
    "    for i in range(tens.shape[0]):\n",
    "        label = tens[i].detach().numpy()\n",
    "        l = []\n",
    "        for j in range(len(label)):\n",
    "            if label[j] > max(label) - eplison:\n",
    "                l.append(j)\n",
    "        l_index.append(l)\n",
    "    return l_index\n",
    "    \n",
    "def maximum_comparison(list1, list2): # list2 is supposed to be real labels with multiple maximum values\n",
    "    tot = 0\n",
    "    for i in range(len(list1)):\n",
    "        for j in range(len(list2[i])):\n",
    "            list2[i][j] = round(list2[i][j])\n",
    "        for j in range(len(list1[i])):\n",
    "            if int(list1[i][j]) in list2[i]:\n",
    "                tot += 1\n",
    "                break\n",
    "    return tot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CerealTimeKillersDataset(Dataset):\n",
    "    \"\"\"Spectrogram dataset for torch\"\"\"\n",
    "\n",
    "    def __init__(self, df, transform = None):\n",
    "        self.ori_dataframe = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ori_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        spectrogram = self.ori_dataframe.iloc[idx, -1]\n",
    "        spectrogram = torch.tensor(spectrogram)\n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        labels = self.ori_dataframe.iloc[idx, :-2]\n",
    "        labels = torch.tensor(labels).type(torch.FloatTensor)\n",
    "        if self.transform:\n",
    "            labels = self.transform(labels)\n",
    "        \n",
    "        quadrants = self.ori_dataframe.iloc[idx, -2]\n",
    "        quadrants = torch.tensor(quadrants).type(torch.FloatTensor)\n",
    "        if self.transform:\n",
    "            quadrants = self.transform(quadrants)\n",
    "        \n",
    "        return (spectrogram, labels, quadrants)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specgram(df, labels, winlen = None, stride = 1, nperseg = 256, fs = 129):\n",
    "    \"\"\"\n",
    "    Spectrogram from EEG data\n",
    "    \n",
    "    Inputs:\n",
    "    df (pandas.DataFrame): EEG dataframe\n",
    "    labels (CerealTimeKillersLabels): Electrode labels used for model prediction\n",
    "    winlen (None/int): Time window for input sampling (for the whole timepoints, Default is None)\n",
    "    stride (int): Temporal leap for input sampling (Default is 1)\n",
    "    nperseg (int): N per seg of spectrogram (Default is 256)\n",
    "    fs (int): Framerate of spectrogram (Default is 128)\n",
    "    \n",
    "    Returns:\n",
    "    data (np.array): EEG data spectrogram with [samplepoint, frequency, time, channel]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load selected electrodes\n",
    "    df = pd.DataFrame(df, columns = labels)\n",
    "    d = np.array(df, dtype = float) # Switching from pandas to numpy array as this might be more comfortable for people\n",
    "    \n",
    "    full_spec = []\n",
    "    for idx, d2 in enumerate(d.T):\n",
    "        _, _, Sxx = spectrogram(d2, nperseg = nperseg, fs = fs)\n",
    "        full_spec.append(Sxx)\n",
    "        \n",
    "    #DIMENSIONS OF FULL_SPEC WITHOUT WINDOWING (I.E. FULL WINDOWING)\n",
    "    #DIMENSION 1: 1                      - FOR DIMENSIONAL CONSISTENCY\n",
    "    #DIMENSION 2: CHANNELS  (DEFAULT=14) - MIGHT CHANGE (SO NOT REALLY DEFAULT BUT OK)\n",
    "    #DIMENSION 3: FREQUENCY (DEFAULT=129)\n",
    "    #DIMENSION 4: TIME      (DEFAULT=170) - MIGHT CHANGE AS WELL OK - WE ARE WORKING ON IT\n",
    "    \n",
    "    full_spec = np.vstack([full_spec])\n",
    "    full_spec = np.moveaxis(full_spec, 0, 0)\n",
    "    if winlen == None:\n",
    "        return np.array([full_spec])\n",
    "    \n",
    "    i = 0\n",
    "    full_spec_wind = []\n",
    "    # STRICK THE FOLLOWING LOOP ON THE TIME (WINDOW) DIMENSION!\n",
    "    while i * stride + winlen < full_spec.shape[2]:\n",
    "        full_spec_wind.append(full_spec[: , : , i * stride : i * stride + winlen])\n",
    "        i += 1\n",
    "    \n",
    "    #DIMENSIONS OF FULL_SPEC WITH WINDOWING    (FULL_SPEC_WIND) \n",
    "    #DIMENSION 1: SAMPLE    (NO DEFAULT - SORRY)\n",
    "    #DIMENSION 2: CHANNELS  (DEFAULT=14) - MIGHT CHANGE (SO NOT REALLY DEFAULT BUT OK)\n",
    "    #DIMENSION 3: FREQUENCY (DEFAULT=129)\n",
    "    #DIMENSION 4: WINDOWS   (DEFAULT=1)\n",
    "    \n",
    "    full_spec_wind = np.array(full_spec_wind)\n",
    "    return full_spec_wind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CerealTimeKillersDataLoader(dir_base, label_class, label_range, \n",
    "                                dataset_mix = True, \n",
    "                                winlen = None, stride = 1, nperseg = 256, fs = 129,\n",
    "                                transform = None):\n",
    "    \"\"\"\n",
    "    Cereal Time Killers Data Loader\n",
    "    \n",
    "    Inputs:\n",
    "    dir_base (str): Working space dictionary\n",
    "    label_class (CerealTimeKillersLabels): Labels used for model prediction\n",
    "    label_range (1*2 list): The [min, max] of emotional states for transformation\n",
    "    dataset_mix (bool): Whether to allow between-subject and between-game dataset mixture (Default is True)\n",
    "    winlen (None/int): Time window for input sampling (for the whole timepoints, Default is None)\n",
    "    stride (int): Temporal leap for input sampling (Default is 1)\n",
    "    nperseg (int): N per seg of spectrogram (Default is 256)\n",
    "    fs (int): Framerate of spectrogram (Default is 128)\n",
    "    transform (torchvision.transforms.transforms.Compose): Torch transormfation (Default is None)\n",
    "    \n",
    "    Returns:\n",
    "    FullDataset (CerealTimeKillersDatase list): full data with EEG spectrogram and fixed labels (information and/or emotional states) in CerealTimeKillersLabels\n",
    "        FullDataset[i]: ith datapoint of [spectrogram, labels, quadrants]\n",
    "    DataSize (Tuple): Data size for single point as (Input size as tuple, Output size as int)\n",
    "    ExpIndex (pandas.DataFrame): Corresponsing ['subject', 'game'] index with shared row indices from FullDataset\n",
    "    \"\"\"\n",
    "    \n",
    "    specgram_name = 'full_specgram_1'\n",
    "    \n",
    "    # Load label & EEG data\n",
    "    labels_df = pd.read_csv(f'{dir_base}GameLabels.csv')\n",
    "    spec_df = pd.DataFrame(columns = label_class.fixed + ['emotion', specgram_name], dtype = float)\n",
    "    index_df = pd.DataFrame(columns = ['subject', 'game'], dtype = int)\n",
    "    \n",
    "    # Create spectrogram dataframe\n",
    "    for idx in range(labels_df.shape[0]): \n",
    "        \n",
    "        # Load info and fixed labels\n",
    "        subject = int(labels_df['subject'].iloc[idx])\n",
    "        game = int(labels_df['game'].iloc[idx])\n",
    "        fixed_labels = labels_df[label_class.fixed].iloc[idx]\n",
    "        fixed_labels = list(np.array(np.array(fixed_labels, dtype = 'float') - label_range[0]) / (label_range[1] - label_range[0]))\n",
    "        \n",
    "        # Maximum quadrant emotion labels\n",
    "        quadrant_labels = labels_df[label_class.quadrant].iloc[idx]\n",
    "        quadrant_labels = list(np.array(quadrant_labels, dtype = 'float'))\n",
    "        \n",
    "        # You can also just paste in the Directory of the csv file - on windows you may have to change the slash direction\n",
    "        DirComb = f'{dir_base}GAMEEMO/(S{str(subject).zfill(2)})/Preprocessed EEG Data/.csv format/S{str(subject).zfill(2)}G{str(game)}AllChannels.csv'\n",
    "        CsvSpec = pd.read_csv(DirComb, sep = ',')\n",
    "        \n",
    "        # Get EEG spectrogram\n",
    "        spec_EEG = get_specgram(CsvSpec, label_class.electrode, \n",
    "                                winlen = winlen, stride = stride, nperseg = nperseg, fs = fs)\n",
    "        \n",
    "        # Add new data to dataframe\n",
    "        new_spec_list, new_index_list = list(), list()\n",
    "        if dataset_mix:\n",
    "            for i in range(spec_EEG.shape[0]):\n",
    "                new_spec_list.append(fixed_labels + [quadrant_labels] + [spec_EEG[i]])\n",
    "                new_index_list.append([subject, game])\n",
    "        else:\n",
    "            new_spec_list.append(fixed_labels + [quadrant_labels] + [spec_EEG])\n",
    "            new_index_list.append([subject, game])\n",
    "        \n",
    "        # Update dataframe\n",
    "        new_spec_df = pd.DataFrame(new_spec_list, columns = label_class.fixed + ['emotion', specgram_name], dtype = float)\n",
    "        spec_df = pd.concat([spec_df, new_spec_df], ignore_index = True)    \n",
    "        new_index_df = pd.DataFrame(new_index_list, columns = ['subject', 'game'], dtype = int)\n",
    "        \n",
    "        index_df = pd.concat([index_df, new_index_df], ignore_index = True)\n",
    "    \n",
    "    # Output\n",
    "    final_df = CerealTimeKillersDataset(df = spec_df, transform = transform)\n",
    "    data_size = (tuple(final_df[0][0].shape), tuple(final_df[0][1].shape))\n",
    "\n",
    "    return final_df, data_size, index_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CerealTimeKillersDataSplitter(full_dataset, exp_index, \n",
    "                                  allocation_test = None, \n",
    "                                  test_ratio = 0.2, target_test = [], k_folds = 10, \n",
    "                                  batch_size_train = 16, batch_size_test = 32, \n",
    "                                  seed = 0, generator = None):\n",
    "    \"\"\"\n",
    "    Cereal Time Killers Data Splitter\n",
    "    \n",
    "    Inputs:\n",
    "    full_dataset (CerealTimeKillersDataset): full data with EEG spectrogram and experimental labels (information and emotional states)\n",
    "        full_dataset[i]: ith data for a specific subject and game of {'spectrogram': spectrogram, 'labels': labels}\n",
    "    exp_index (pandas.DataFrame): Corresponsing ['subject', 'game', 'emotion'] with shared row indices from full_dataset\n",
    "    allocation_test (None/str): Which to be based for allocating testing dataset (Default is None) # [None, 'subject', 'game']\n",
    "    test_ratio (float) Proportion of data used for testing when Allocation_test == None (Default is 0.2)\n",
    "    target_test (list): Int list for allocating corresponding game/subject as testing dataset when Allocation_test != None (Default is [])\n",
    "    k_folds (int): Number for K-folds for training vs validation (Default is 10)\n",
    "    batch_size_train (int): Number of examples per minibatch during training (Default is 16)\n",
    "    batch_size_test (int): Number of examples per minibatch during validation/testing (Default is 1)\n",
    "    seed (int): Random seed for reproducibility (Default is 0)\n",
    "    generator (torch._C.Generator): Torch generator for reproducibility (Default is None)\n",
    "    \n",
    "    Returns:\n",
    "    SplittedDataset (dict): Full dataset splitted in {'train': training, 'val': validation, 'test': testing}\n",
    "        SplittedDataset['train'][fold].dataset (CerealTimeKillersDataset): Training dataset in nth fold\n",
    "        SplittedDataset['val'][fold].dataset (CerealTimeKillersDataset): Validation dataset in nth fold\n",
    "        SplittedDataset['test'].dataset (CerealTimeKillersDataset): Testing dataset outside folds\n",
    "    SplittedDataLength (dict): Length of dataset in {'train': training, 'val': validation, 'test': testing}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split into train/val and test datasets\n",
    "    train_set_index, test_set_index = list(), list()\n",
    "    if allocation_test == None:\n",
    "        test_size = int(test_ratio * len(full_dataset))\n",
    "        train_size = len(full_dataset) - test_size\n",
    "        train_set_orig, test_set_orig = random_split(full_dataset, \n",
    "                                                     [train_size, test_size], \n",
    "                                                     generator = generator)\n",
    "    elif (allocation_test == 'subject') or (allocation_test == 'game'):\n",
    "        train_set_index = exp_index[~exp_index[allocation_test].isin(target_test)].index.tolist()\n",
    "        test_set_index = exp_index[exp_index[allocation_test].isin(target_test)].index.tolist()\n",
    "        train_set_orig = Subset(full_dataset, train_set_index)\n",
    "        test_set_orig = Subset(full_dataset, test_set_index)\n",
    "    else:\n",
    "        print(\"Allocate testing dataset based on one of the 'Subject', 'Game', or None.\")\n",
    "        return None\n",
    "    \n",
    "    # Test dataset loader\n",
    "    test_loader = DataLoader(test_set_orig,\n",
    "                             batch_size = batch_size_test,\n",
    "                             num_workers = 0,\n",
    "                             generator = g_seed)\n",
    "    \n",
    "    # K-fold Cross Validator\n",
    "    train_loader, val_loader = [[]] * k_folds, [[]] * k_folds\n",
    "    kfold = KFold(n_splits = k_folds, shuffle = True, random_state = seed)\n",
    "    for fold, (train_i, val_i) in enumerate(kfold.split(train_set_orig)):\n",
    "        \n",
    "        # Sample train/test dataset from indices\n",
    "        train_sampler = SubsetRandomSampler(train_i, generator = g_seed)\n",
    "        val_sampler = SubsetRandomSampler(val_i, generator = g_seed)\n",
    "        \n",
    "        # Train/Validation dataset loader\n",
    "        train_loader[fold] = DataLoader(train_set_orig,\n",
    "                                        sampler = train_sampler,\n",
    "                                        batch_size = batch_size_train,\n",
    "                                        num_workers = 0,\n",
    "                                        generator = generator)\n",
    "        val_loader[fold] = DataLoader(train_set_orig,\n",
    "                                      sampler = val_sampler,\n",
    "                                      batch_size = batch_size_test,\n",
    "                                      num_workers = 0,\n",
    "                                      generator = generator)\n",
    "    \n",
    "    # return datasplitter\n",
    "    data_loader = {'train': train_loader, 'val': val_loader, 'test': test_loader}\n",
    "    data_length = {'train': len(train_sampler), 'val': len(val_sampler), 'test': len(test_set_orig)}\n",
    "    return data_loader, data_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_loader, optimizer = None, criterion = nn.MSELoss()):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for (data, target, quadrant) in train_loader:\n",
    "        data, target = data.type(torch.float).to(args['device']), target.type(torch.float).to(args['device'])\n",
    "        optimizer.zero_grad() \n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target) + args['l1'] * L1_norm(model) + args['l2'] * L2_norm(model)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, label, model, test_loader, criterion = nn.MSELoss()):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    global Is_2D_to_quardrant_emotion\n",
    "    eval_loss = 0.0\n",
    "    acc = 0.0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (data, target, quadrant) in test_loader:\n",
    "            data = data.type(torch.float).to(args['device'])\n",
    "            target = target.type(torch.float).to(args['device'])\n",
    "            quadrant = quadrant.type(torch.float).to(args['device'])\n",
    "            output = model(data)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            if not Is_2D_to_quardrant_emotion:\n",
    "                predicted = maximum_extraction(output)\n",
    "                labels = maximum_extraction(target)\n",
    "            else:\n",
    "                predicted = emotion_transformation(output, label)\n",
    "                # labels = maximum_extraction(quadrant)\n",
    "                labels = emotion_transformation(target, label)\n",
    "            acc += maximum_comparison(predicted, labels)\n",
    "            total += target.size(0)\n",
    "            \n",
    "    return eval_loss / len(test_loader), acc * 100 / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(args, label, model, train_loader, val_loader, test_loader, \n",
    "               optimizer = None, criterion = nn.MSELoss()):\n",
    "    \n",
    "    model = model.to(args['device'])\n",
    "    \n",
    "    val_loss_list, train_loss_list, test_loss_list = [], [], []\n",
    "    val_acc_list, train_acc_list, test_acc_list = [], [], []\n",
    "    param_norm_list = []\n",
    "    best_loss = 100\n",
    "    for epoch in tqdm(range(args['epochs'])):\n",
    "        \n",
    "        train(args, model, train_loader, optimizer = optimizer, criterion = criterion)\n",
    "        param_norm = calculate_frobenius_norm(model)\n",
    "        \n",
    "        train_loss, train_acc = test(args, label, model, train_loader, criterion = criterion)\n",
    "        val_loss, val_acc = test(args, label, model, val_loader, criterion = criterion)\n",
    "        test_loss, test_acc = test(args, label, model, test_loader, criterion = criterion)\n",
    "        \n",
    "        if (val_loss < best_loss) or (epoch == 0):\n",
    "            best_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model)\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "        val_acc_list.append(val_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        param_norm_list.append(param_norm)\n",
    "        \n",
    "        if wait > args['patience']:\n",
    "            print('Early stopped on epoch', best_epoch)\n",
    "            break\n",
    "        \n",
    "        if ((epoch + 1) % 10 == 0) or (epoch + 1 == args['epochs']):\n",
    "            print('-----Epoch ', epoch + 1, '/', args['epochs'])\n",
    "            print('Train/Val/TEST MSE:', train_loss, val_loss, test_loss)\n",
    "            print('Train/Val/TEST Accuracy:', train_acc, val_acc, test_acc)\n",
    "    \n",
    "    plot_loss_accuracy(train_loss_list, val_loss_list, test_loss_list, \n",
    "                       train_acc_list, val_acc_list, test_acc_list)\n",
    "\n",
    "    return (train_loss_list, val_loss_list, test_loss_list), (train_acc_list, val_acc_list, test_acc_list), param_norm_list, best_model, best_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main function of model simulation\n",
    "def CerealTimeKillersModelSimulator(args, label,\n",
    "                                    TrainDataLoader, ValDataLoader, TestDataLoader, DataSize,\n",
    "                                    K_fold_train = False, k_folds = 1):\n",
    "    \n",
    "    N_FOLD = k_folds if K_fold_train else 1\n",
    "    loss, acc, param, models = [], [], [], []\n",
    "    \n",
    "    for fold in range(N_FOLD):\n",
    "        print('\\n%d/%d Fold' % (fold + 1, N_FOLD))\n",
    "        print('----------------------------')\n",
    "    \n",
    "        model, optimizer, criterion = CerealTimeKillersModelGenerator(args, size = DataSize)\n",
    "        loss_list, acc_list, param_norm_list, trained_model, epoch = simulation(args, label, model,\n",
    "                                                                                TrainDataLoader[fold],\n",
    "                                                                                ValDataLoader[fold],\n",
    "                                                                                TestDataLoader,\n",
    "                                                                                optimizer = optimizer,\n",
    "                                                                                criterion = criterion)\n",
    "    \n",
    "        loss_list, acc_list = np.array(loss_list), np.array(acc_list)\n",
    "        loss.append([loss_list[0, epoch], loss_list[1, epoch], loss_list[2, epoch]])\n",
    "        acc.append([acc_list[0, epoch], acc_list[1, epoch], acc_list[2, epoch]])\n",
    "        param.append(param_norm_list[epoch])\n",
    "        models.append(trained_model)\n",
    "        \n",
    "        print('Train/Val/Test Final MSE:', list(loss[-1]))\n",
    "        print('Train/Val/Test Maximum Accuracy:', list(acc[-1]))\n",
    "    \n",
    "    return loss, acc, param, models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model settings - Change models here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTK_CRNN_Net(nn.Module):\n",
    "    def __init__(self, input_shape, out_size):\n",
    "\n",
    "        super(CTK_CRNN_Net, self).__init__()\n",
    "        \n",
    "        # Model hyperparametres (layer by layer)\n",
    "        conv_channel = [5, 5]\n",
    "        conv_kernel = [(3, 3), (5, 3)]\n",
    "        pool_kernel = [(1, 1), (1, 1)]\n",
    "        rnn_layer = [1]\n",
    "        rnn_unit = [64]\n",
    "        rnn_drop = [0]\n",
    "        fc_unit = []\n",
    "        drop_out = []\n",
    "        \n",
    "        self.rnn_layer = rnn_layer\n",
    "        self.rnn_unit = rnn_unit\n",
    "        \n",
    "        # Hidden layers\n",
    "        img_size = np.array(input_shape[2:])\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = input_shape[1], out_channels = conv_channel[0], kernel_size = conv_kernel[0])\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = pool_kernel[0])\n",
    "        img_size = np.floor((img_size - np.array(conv_kernel[0]) + 1.0) / np.array(pool_kernel[0]))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = conv_channel[0], out_channels = conv_channel[1], kernel_size = conv_kernel[1])\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = pool_kernel[1])\n",
    "        img_size = np.floor((img_size - np.array(conv_kernel[1]) + 1.0) / np.array(pool_kernel[1]))\n",
    "        \n",
    "        fc_input_size = np.int(np.prod(img_size) * conv_channel[1])\n",
    "        \n",
    "        self.rnn1 = nn.LSTM(input_size = fc_input_size, hidden_size = rnn_unit[0], num_layers = rnn_layer[0], dropout = rnn_drop[0], batch_first = True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = rnn_unit[0] * rnn_layer[0], out_features = out_size[0])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        shape = x.shape\n",
    "        x = x.view(shape[0] * shape[1], shape[2], shape[3], shape[4])\n",
    "        \n",
    "        hidden = (torch.randn(self.rnn_layer[0], shape[0], self.rnn_unit[0]), \n",
    "                  torch.randn(self.rnn_layer[0], shape[0], self.rnn_unit[0]))   # for LSTM\n",
    "        # hidden = torch.randn(self.rnn_layer[0], shape[0], self.rnn_unit[0])   # for simple RNN and GRU\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)        \n",
    "        x = x.view(shape[0], shape[1], -1)\n",
    "        _, x = self.rnn1(x, hidden) # use final hidden state for non-series label prediction!\n",
    "        x = x[0].permute(1, 0, 2) # for LSTM\n",
    "        # x = x.permute(1, 0, 2) # for simple RNN and GRU\n",
    "        x = F.relu(x)\n",
    "        x = x.contiguous().view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourNet(nn.Module):\n",
    "    def __init__(self, input_shape, out_size):\n",
    "\n",
    "        super(YourNet, self).__init__()\n",
    "        \n",
    "        # Model hyperparametres (layer by layer)\n",
    "        ...\n",
    "        \n",
    "        # Hidden layers\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection function\n",
    "def CerealTimeKillersModelGenerator(args, size):\n",
    "    \n",
    "    model = CTK_CRNN_Net(input_shape = size[0], out_size = size[1])\n",
    "    optimizer = optim.SGD(model.parameters(), lr = args['lr'], momentum = args['momentum'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    return model, optimizer, criterion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input settings - Change hypermatres here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CerealTimeKillersLabels:\n",
    "    \"\"\"\n",
    "    Select labels for model prediction\n",
    "    Labels used for prediction: info + electrode --> prediction\n",
    "    CHANGE these with necessity before loading data\n",
    "    \"\"\"\n",
    "    \n",
    "    # ['subject', 'game', 'gender', 'age', 'disturbance', 'experience', 'memory']\n",
    "    info = []\n",
    "        \n",
    "    # ['AF3', 'AF4', 'F3', 'F4', 'F7', 'F8', 'FC5', 'FC6', 'O1', 'O2', 'P7', 'P8', 'T7', 'T8']\n",
    "    electrode = ['AF3', 'AF4', 'F3', 'F4', 'F7', 'F8', 'FC5', 'FC6', 'O1', 'O2', 'P7', 'P8', 'T7', 'T8']\n",
    "        \n",
    "    # ['satisfied', 'boring', 'horrible', 'calm', 'funny', 'valence', 'arousal']\n",
    "    prediction = ['boring', 'horrible', 'calm', 'funny']\n",
    "    # prediction = ['valence', 'arousal']\n",
    "    \n",
    "    # Quadrant emotions (applied after predicting valence/arousal)\n",
    "    quadrant = ['boring', 'horrible', 'calm', 'funny']\n",
    "    \n",
    "    # Fixed variables\n",
    "    fixed = info + prediction\n",
    "    \n",
    "    # Summarise labels for model\n",
    "    label = info + electrode + prediction\n",
    "\n",
    "\n",
    "# 2D emotions ['valence', 'arousal'] to 4 quadrant emotions ['boring', 'horrible', 'calm', 'funny']\n",
    "def emotion_transformation(pred, label, c = 0.5):\n",
    "    # Quadrant I:   ('valence' >= c, 'arousal' >= c) --> 'funny' \n",
    "    # Quadrant II:  ('valence' >= c, 'arousal' <= c) --> 'calm'\n",
    "    # Quadrant III: ('valence' <= c, 'arousal' <= c) --> 'boring'\n",
    "    # Quadrant IV:  ('valence' <= c, 'arousal' >= c) --> 'horrible'\n",
    "    ans = []\n",
    "    pred = pred.detach().numpy()\n",
    "    i, j = 0, 1 # index of 'valence' and 'arousal' in CerealTimeKillersLabels.prediction\n",
    "    \n",
    "    for k in range(pred.shape[0]): # batch\n",
    "        anss = []\n",
    "        if (pred[k, i] >= c) and (pred[k, j] >= c):\n",
    "            anss.append(3) # index of 'funny' in CerealTimeKillersLabels.quadrant\n",
    "        if (pred[k, i] >= c) and (pred[k, j] <= c):\n",
    "            anss.append(2) # index of 'calm' in CerealTimeKillersLabels.quadrant\n",
    "        if (pred[k, i] <= c) and (pred[k, j] >= c):\n",
    "            anss.append(1) # index of 'horrible' in CerealTimeKillersLabels.quadrant\n",
    "        if (pred[k, i] <= c) and (pred[k, j] <= c):\n",
    "            anss.append(0) # index of 'boring' in CerealTimeKillersLabels.quadrant\n",
    "        ans.append(anss)\n",
    "        \n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -> `Change runtime type.`  select `GPU` \n",
      "Current device: cpu\n",
      "Random seed 2021 has been set.\n"
     ]
    }
   ],
   "source": [
    "# General settings\n",
    "workspace_dir = '' # Workspace directionary\n",
    "LabelRange = [1, 9] # The [min, max] of emotional states for transformation\n",
    "\n",
    "# Whether to allow between-window dataset mixture\n",
    "# SET TO FALSE FOR 4-DIMENSIONAL INPUT WHEN USING RNN\n",
    "Is_between_subject = False # Default is True for 3-dimensional input\n",
    "\n",
    "# Whether to transform 2D emotion  (valence/arousal) to 4 quardrant emotions ()\n",
    "Is_2D_to_quardrant_emotion = True # Default is False\n",
    "\n",
    "# Which to be based for allocating testing dataset (only when Is_between_subject = True)\n",
    "Allocation_test = None # [None, 'subject', 'game'] # Default is None\n",
    "test_ratio = 0.2 # Proportion of data used for testing when Allocation_test == None\n",
    "Target_test = [25, 26, 27] # Int list for allocating corresponding game/subject as testing dataset when Allocation_test != None\n",
    "\n",
    "# Model structural settings\n",
    "N_inputtime = 20 # Time window for input sampling (Default is None for the whole timepoints)\n",
    "N_stridetime = 10 # Temporal leap for input sampling when N_inputtime != None\n",
    "N_perseg = 256 # N per seg of spectrogram\n",
    "N_framerate = 128 # Framerate of spectrogram\n",
    "\n",
    "# Model training settings\n",
    "batch_size_train = 16 # Number of examples per minibatch during training\n",
    "batch_size_test = 1 # Number of examples per minibatch during validation/testing\n",
    "k_folds = 5 # Number for K-folds for training vs validation (validation is 1/k_folds of the train/val set)\n",
    "K_fold_train = True # Whether enable the full K-fold cross-validation for training (if False, validate only once)\n",
    "\n",
    "# Model hypermparametres\n",
    "args = {\n",
    "    'epochs': 300,\n",
    "    'lr': 5e-3,\n",
    "    'momentum': 0.99,\n",
    "    'l1': 1e-3,\n",
    "    'l2': 1e-3,\n",
    "    'patience': 50,\n",
    "    'device': set_device(),\n",
    "}\n",
    "print('Current device:', args['device'])\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 2021\n",
    "set_seed(seed = SEED)\n",
    "g_seed = torch.Generator()\n",
    "g_seed.manual_seed(SEED)\n",
    "\n",
    "# Torch-based data transformation\n",
    "data_transform = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: {'train': 70, 'val': 17, 'test': 21}\n",
      "Input shape: [channel, frequency, time]\n",
      "Single input data size: (15, 14, 129, 20)\n",
      "Single output data size: (4,)\n"
     ]
    }
   ],
   "source": [
    "# Implement Dataloader\n",
    "FullDataset, DataSize, ExpIndex = CerealTimeKillersDataLoader(dir_base = workspace_dir,\n",
    "                                                              label_class = CerealTimeKillersLabels,\n",
    "                                                              label_range = LabelRange,\n",
    "                                                              dataset_mix = Is_between_subject,\n",
    "                                                              winlen = N_inputtime,\n",
    "                                                              stride = N_stridetime,\n",
    "                                                              nperseg = N_perseg,\n",
    "                                                              fs = N_framerate,\n",
    "                                                              transform = data_transform)\n",
    "\n",
    "# Implement DataSplitter\n",
    "SplittedDataset, SplittedDataLength = CerealTimeKillersDataSplitter(FullDataset, \n",
    "                                                                    exp_index = ExpIndex, \n",
    "                                                                    allocation_test = Allocation_test,\n",
    "                                                                    test_ratio = test_ratio,\n",
    "                                                                    target_test = Target_test,\n",
    "                                                                    k_folds = k_folds,\n",
    "                                                                    batch_size_train = batch_size_train,\n",
    "                                                                    batch_size_test = batch_size_test,\n",
    "                                                                    seed = SEED,\n",
    "                                                                    generator = g_seed)\n",
    "\n",
    "# Load Splited data\n",
    "(TrainDataLoader, ValDataLoader, TestDataLoader) = (SplittedDataset['train'],\n",
    "                                                    SplittedDataset['val'],\n",
    "                                                    SplittedDataset['test'])\n",
    "\n",
    "# Show data size\n",
    "print('Dataset length:', SplittedDataLength)\n",
    "print('Input shape: [channel, frequency, time]')\n",
    "print('Single input data size:', DataSize[0])\n",
    "print('Single output data size:', DataSize[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTK_CRNN_Net(\n",
      "  (conv1): Conv2d(14, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(5, 5, kernel_size=(5, 3), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (rnn1): LSTM(9840, 64, batch_first=True)\n",
      "  (fc1): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model selection\n",
    "model, optimizer, criterion = CerealTimeKillersModelGenerator(args, size = DataSize)\n",
    "print(model)\n",
    "# summary(model, DataSize[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/5 Fold\n",
      "----------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e725d4119d974bcaa076710a8e6436b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chronowanderer/.local/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/Users/chronowanderer/.local/lib/python3.7/site-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch  10 / 300\n",
      "Train/Val/TEST MSE: 0.13345983028411865 0.12235350348055363 0.1405422061326958\n",
      "Train/Val/TEST Accuracy: 53.6231884057971 66.66666666666667 28.571428571428573\n",
      "-----Epoch  20 / 300\n",
      "Train/Val/TEST MSE: 0.1258102148771286 0.10496219082011117 0.14539631263219885\n",
      "Train/Val/TEST Accuracy: 53.6231884057971 66.66666666666667 28.571428571428573\n",
      "-----Epoch  30 / 300\n",
      "Train/Val/TEST MSE: 0.11472487598657607 0.11064199337528811 0.1456965486341644\n",
      "Train/Val/TEST Accuracy: 53.6231884057971 66.66666666666667 28.571428571428573\n",
      "-----Epoch  40 / 300\n",
      "Train/Val/TEST MSE: 0.11290243417024612 0.11181925464835432 0.1447343806337033\n",
      "Train/Val/TEST Accuracy: 53.6231884057971 66.66666666666667 28.571428571428573\n",
      "-----Epoch  50 / 300\n",
      "Train/Val/TEST MSE: 0.11232930868864059 0.10786326074351867 0.13931513284998281\n",
      "Train/Val/TEST Accuracy: 56.52173913043478 66.66666666666667 28.571428571428573\n",
      "-----Epoch  60 / 300\n",
      "Train/Val/TEST MSE: 0.09933212250471116 0.10399296393411027 0.12763507420285827\n",
      "Train/Val/TEST Accuracy: 60.869565217391305 66.66666666666667 28.571428571428573\n",
      "-----Epoch  70 / 300\n",
      "Train/Val/TEST MSE: 0.08137103468179703 0.10385122812456554 0.12136966877040409\n",
      "Train/Val/TEST Accuracy: 69.56521739130434 66.66666666666667 38.095238095238095\n",
      "-----Epoch  80 / 300\n",
      "Train/Val/TEST MSE: 0.06033311039209366 0.10003992097659244 0.10528257192068156\n",
      "Train/Val/TEST Accuracy: 75.3623188405797 66.66666666666667 42.857142857142854\n"
     ]
    }
   ],
   "source": [
    "# Model simulation\n",
    "loss_K, acc_K, param_K, models_K = CerealTimeKillersModelSimulator(args, CerealTimeKillersLabels, \n",
    "                                                                   TrainDataLoader,\n",
    "                                                                   ValDataLoader,\n",
    "                                                                   TestDataLoader,\n",
    "                                                                   DataSize,\n",
    "                                                                   K_fold_train = K_fold_train,\n",
    "                                                                   k_folds = k_folds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average results from K-folds\n",
    "print('Train/Val/Test Average MSE:', list(np.mean(np.array(loss_K), axis = 0)))\n",
    "print('Train/Val/Test Average Accuracy:', list(np.mean(np.array(acc_K), axis = 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print single prediction results from data loader\n",
    "fold = 0\n",
    "ShowDataset = TestDataLoader\n",
    "ShowModel = models_K[fold]\n",
    "BATCH_SHOW = 20\n",
    "\n",
    "ShowModel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (data, target, quadrant) in enumerate(ShowDataset):\n",
    "        data = data.type(torch.float).to(args['device'])\n",
    "        target = target.type(torch.float).to(args['device'])\n",
    "        quadrant = quadrant.type(torch.float).to(args['device'])\n",
    "        output = ShowModel(data)\n",
    "            \n",
    "        eval_loss = criterion(output, target).item()\n",
    "        \n",
    "        if not Is_2D_to_quardrant_emotion:\n",
    "            predicted = maximum_extraction(output)\n",
    "            labels = maximum_extraction(target)\n",
    "        else:\n",
    "            predicted = emotion_transformation(output, CerealTimeKillersLabels)\n",
    "            flabels = maximum_extraction(quadrant)\n",
    "            labels = emotion_transformation(target, CerealTimeKillersLabels)\n",
    "        \n",
    "        eval_acc = maximum_comparison(predicted, labels) * 100.0 / target.size(0)\n",
    "        \n",
    "        if idx < BATCH_SHOW:\n",
    "            print('Batch', idx + 1, ' ( Size', target.size(0), '):')\n",
    "            print('Output Example:', output[0].detach().numpy(), 'with label ', predicted[0])\n",
    "            if not Is_2D_to_quardrant_emotion:\n",
    "                print('Target Example:', target[0].detach().numpy(), 'with label ', labels[0])\n",
    "            else:\n",
    "                print('Target Example:', target[0].detach().numpy(), 'with label ', flabels[0])\n",
    "                print('Real Label Example:', quadrant[0].detach().numpy(), 'with label ', labels[0])\n",
    "            print('------- MSE:', eval_loss, ' Accuracy:', eval_acc, '%-------\\n')\n",
    "        else:\n",
    "            print('Etc. for totally ', len(ShowDataset), 'batches.')\n",
    "            break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
