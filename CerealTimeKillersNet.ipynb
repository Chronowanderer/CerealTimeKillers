{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3fee1f",
   "metadata": {},
   "source": [
    "# CerealTimeKillersNet: Deep neural network for emotional states predictions from EEG data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88c0c9",
   "metadata": {},
   "source": [
    "## Package setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e68deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Math packages\n",
    "import math\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "# Plot packages\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, SubsetRandomSampler, random_split\n",
    "\n",
    "# ML Packages\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd672a",
   "metadata": {},
   "source": [
    "## Basic functions setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403a8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "# Executing `set_seed(seed=seed)` you are setting the seed to ensure reproducibility.\n",
    "# for DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "def set_seed(seed = None, seed_torch = True):\n",
    "    if seed is None:\n",
    "        seed = np.random.choice(2 ** 32)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if seed_torch:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    print(f'Random seed {seed} has been set.')\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9f6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()` especially if torch modules used.\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"WARNING: For this notebook to perform best, \"\n",
    "            \"if possible, in the menu under `Runtime` -> \"\n",
    "            \"`Change runtime type.`  select `GPU` \")\n",
    "    else:\n",
    "        print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c574c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot settings\n",
    "\n",
    "def plot_loss_accuracy(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc):\n",
    "    epochs = len(train_loss)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.plot(list(range(epochs)), train_loss, label = 'Training')\n",
    "    ax1.plot(list(range(epochs)), val_loss, label = 'Validation')\n",
    "    ax1.plot(list(range(epochs)), test_loss, label = 'Testing')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Epoch vs Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(list(range(epochs)), train_acc, label = 'Training')\n",
    "    ax2.plot(list(range(epochs)), val_acc, label = 'Validation')\n",
    "    ax2.plot(list(range(epochs)), test_acc, label = 'Testing')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Epoch vs Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    fig.set_size_inches(15.5, 5.5)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01748ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norm\n",
    "def calculate_frobenius_norm(model):\n",
    "    norm = 0.0\n",
    "    for param in model.parameters():\n",
    "        norm += torch.sum(param ** 2)\n",
    "    norm = norm ** 0.5\n",
    "    return norm\n",
    "\n",
    "def L1_norm(model):\n",
    "    return sum(p.abs().sum() for p in model.parameters())\n",
    "\n",
    "def L2_norm(model):\n",
    "    return sum((p**2).sum() for p in model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3118c05e",
   "metadata": {},
   "source": [
    "## DataLoader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14c4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CerealTimeKillersDataset(Dataset):\n",
    "    \"\"\"Spectrogram dataset for torch\"\"\"\n",
    "\n",
    "    def __init__(self, df, transform = None):\n",
    "        self.ori_dataframe = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ori_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        spectrogram = self.ori_dataframe.iloc[idx, -1]\n",
    "        spectrogram = torch.tensor(spectrogram)\n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        labels = self.ori_dataframe.iloc[idx, :-1]\n",
    "        labels = torch.tensor(labels).type(torch.FloatTensor)\n",
    "        if self.transform:\n",
    "            labels = self.transform(labels)\n",
    "        \n",
    "        return (spectrogram, labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b494021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specgram(df, labels, winlen = None, stride = 1, nperseg = 256, fs = 129):\n",
    "    \"\"\"\n",
    "    Spectrogram from EEG data\n",
    "    \n",
    "    Inputs:\n",
    "    df (pandas.DataFrame): EEG dataframe\n",
    "    labels (CerealTimeKillersLabels): Electrode labels used for model prediction\n",
    "    winlen (None/int): Time window for input sampling (for the whole timepoints, Default is None)\n",
    "    stride (int): Temporal leap for input sampling (Default is 1)\n",
    "    nperseg (int): N per seg of spectrogram (Default is 256)\n",
    "    fs (int): Framerate of spectrogram (Default is 128)\n",
    "    \n",
    "    Returns:\n",
    "    data (np.array): EEG data spectrogram with [samplepoint, frequency, time, channel]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load selected electrodes\n",
    "    df = pd.DataFrame(df, columns = labels)\n",
    "    d = np.array(df, dtype = float) # Switching from pandas to numpy array as this might be more comfortable for people\n",
    "    \n",
    "    full_spec = []\n",
    "    for idx, d2 in enumerate(d.T):\n",
    "        _, _, Sxx = spectrogram(d2, nperseg = nperseg, fs = fs)\n",
    "        full_spec.append(Sxx)\n",
    "        \n",
    "    #DIMENSIONS OF FULL_SPEC WITHOUT WINDOWING (I.E. FULL WINDOWING)\n",
    "    #DIMENSION 1: 1                      - FOR DIMENSIONAL CONSISTENCY\n",
    "    #DIMENSION 2: CHANNELS  (DEFAULT=14) - MIGHT CHANGE (SO NOT REALLY DEFAULT BUT OK)\n",
    "    #DIMENSION 3: FREQUENCY (DEFAULT=129)\n",
    "    #DIMENSION 4: TIME      (DEFAULT=170) - MIGHT CHANGE AS WELL OK - WE ARE WORKING ON IT\n",
    "    \n",
    "    full_spec = np.vstack([full_spec])\n",
    "    full_spec = np.moveaxis(full_spec, 0, 0)\n",
    "    if winlen == None:\n",
    "        return np.array([full_spec])\n",
    "    \n",
    "    i = 0\n",
    "    full_spec_wind = []\n",
    "    # STRICK THE FOLLOWING LOOP ON THE TIME (WINDOW) DIMENSION!\n",
    "    while i * stride + winlen < full_spec.shape[2]:\n",
    "        full_spec_wind.append(full_spec[: , : , i * stride : i * stride + winlen])\n",
    "        i += 1\n",
    "    \n",
    "    #DIMENSIONS OF FULL_SPEC WITH WINDOWING    (FULL_SPEC_WIND) \n",
    "    #DIMENSION 1: SAMPLE    (NO DEFAULT - SORRY)\n",
    "    #DIMENSION 2: CHANNELS  (DEFAULT=14) - MIGHT CHANGE (SO NOT REALLY DEFAULT BUT OK)\n",
    "    #DIMENSION 3: FREQUENCY (DEFAULT=129)\n",
    "    #DIMENSION 4: WINDOWS   (DEFAULT=1)\n",
    "    \n",
    "    full_spec_wind = np.array(full_spec_wind)\n",
    "    return full_spec_wind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c48df7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CerealTimeKillersDataLoader(dir_base, label_class, label_range, \n",
    "                                dataset_mix = True, \n",
    "                                winlen = None, stride = 1, nperseg = 256, fs = 129,\n",
    "                                transform = None):\n",
    "    \"\"\"\n",
    "    Cereal Time Killers Data Loader\n",
    "    \n",
    "    Inputs:\n",
    "    dir_base (str): Working space dictionary\n",
    "    label_class (CerealTimeKillersLabels): Labels used for model prediction\n",
    "    label_range (1*2 list): The [min, max] of emotional states for transformation\n",
    "    dataset_mix (bool): Whether to allow between-subject and between-game dataset mixture (Default is True)\n",
    "    winlen (None/int): Time window for input sampling (for the whole timepoints, Default is None)\n",
    "    stride (int): Temporal leap for input sampling (Default is 1)\n",
    "    nperseg (int): N per seg of spectrogram (Default is 256)\n",
    "    fs (int): Framerate of spectrogram (Default is 128)\n",
    "    transform (torchvision.transforms.transforms.Compose): Torch transormfation (Default is None)\n",
    "    \n",
    "    Returns:\n",
    "    FullDataset (CerealTimeKillersDatase list): full data with EEG spectrogram and fixed labels (information and/or emotional states) in CerealTimeKillersLabels\n",
    "        FullDataset[i]: ith datapoint of [spectrogram, labels]\n",
    "    DataSize (Tuple): Data size for single point as (Input size as tuple, Output size as int)\n",
    "    ExpIndex (pandas.DataFrame): Corresponsing subject and game (as two columns) with shared row indices from FullDataset\n",
    "    \"\"\"\n",
    "    \n",
    "    specgram_name = 'full_specgram_1'\n",
    "    \n",
    "    # Load label & EEG data\n",
    "    labels_df = pd.read_csv(f'{dir_base}GameLabels.csv')\n",
    "    spec_df = pd.DataFrame(columns = label_class.fixed + [specgram_name], dtype = float)\n",
    "    index_df = pd.DataFrame(columns = ['subject', 'game'], dtype = int)\n",
    "    \n",
    "    # Create spectrogram dataframe\n",
    "    for idx in range(labels_df.shape[0]): \n",
    "        \n",
    "        # Load info and fixed labels\n",
    "        subject = labels_df['subject'].iloc[idx]\n",
    "        game = labels_df['game'].iloc[idx]\n",
    "        fixed_labels = labels_df[label_class.fixed].iloc[idx]\n",
    "        fixed_labels = list(np.array(np.array(fixed_labels, dtype = 'float') - label_range[0]) / (label_range[1] - label_range[0]))\n",
    "        \n",
    "        # You can also just paste in the Directory of the csv file - on windows you may have to change the slash direction\n",
    "        DirComb = f'{dir_base}GAMEEMO/(S{str(subject).zfill(2)})/Preprocessed EEG Data/.csv format/S{str(subject).zfill(2)}G{str(game)}AllChannels.csv'\n",
    "        CsvSpec = pd.read_csv(DirComb, sep = ',')\n",
    "        \n",
    "        # Get EEG spectrogram\n",
    "        spec_EEG = get_specgram(CsvSpec, label_class.electrode, \n",
    "                                winlen = winlen, stride = stride, nperseg = nperseg, fs = fs)\n",
    "        \n",
    "        # Add new data to dataframe\n",
    "        new_spec_list, new_index_list = list(), list()\n",
    "        if dataset_mix:\n",
    "            for i in range(spec_EEG.shape[0]):\n",
    "                new_spec_list.append(fixed_labels + [spec_EEG[i]])\n",
    "                new_index_list.append([subject, game])\n",
    "        else:\n",
    "            new_spec_list.append(fixed_labels + [spec_EEG])\n",
    "            new_index_list.append([subject, game])\n",
    "        \n",
    "        # Update dataframe\n",
    "        new_spec_df = pd.DataFrame(new_spec_list, columns = label_class.fixed + [specgram_name], dtype = float)\n",
    "        spec_df = pd.concat([spec_df, new_spec_df], ignore_index = True)    \n",
    "        new_index_df = pd.DataFrame(new_index_list, columns = ['subject', 'game'], dtype = int)\n",
    "        index_df = pd.concat([index_df, new_index_df], ignore_index = True)\n",
    "    \n",
    "    # Output\n",
    "    final_df = CerealTimeKillersDataset(df = spec_df, transform = transform)\n",
    "    data_size = (tuple(final_df[0][0].shape), tuple(final_df[0][1].shape))\n",
    "\n",
    "    return final_df, data_size, index_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88a2a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CerealTimeKillersDataSplitter(full_dataset, exp_index, \n",
    "                                  allocation_test = None, \n",
    "                                  test_ratio = 0.2, target_test = [], k_folds = 10, \n",
    "                                  batch_size_train = 16, batch_size_test = 32, \n",
    "                                  seed = 0, generator = None):\n",
    "    \"\"\"\n",
    "    Cereal Time Killers Data Splitter\n",
    "    \n",
    "    Inputs:\n",
    "    full_dataset (CerealTimeKillersDataset): full data with EEG spectrogram and experimental labels (information and emotional states)\n",
    "        full_dataset[i]: ith data for a specific subject and game of {'spectrogram': spectrogram, 'labels': labels}\n",
    "    exp_index (pandas.DataFrame): Corresponsing subject and game (as two columns) with shared row indices from full_dataset\n",
    "    allocation_test (None/str): Which to be based for allocating testing dataset (Default is None) # [None, 'subject', 'game']\n",
    "    test_ratio (float) Proportion of data used for testing when Allocation_test == None (Default is 0.2)\n",
    "    target_test (list): Int list for allocating corresponding game/subject as testing dataset when Allocation_test != None (Default is [])\n",
    "    k_folds (int): Number for K-folds for training vs validation (Default is 10)\n",
    "    batch_size_train (int): Number of examples per minibatch during training (Default is 16)\n",
    "    batch_size_test (int): Number of examples per minibatch during validation/testing (Default is 1)\n",
    "    seed (int): Random seed for reproducibility (Default is 0)\n",
    "    generator (torch._C.Generator): Torch generator for reproducibility (Default is None)\n",
    "    \n",
    "    Returns:\n",
    "    SplittedDataset (dict): Full dataset splitted in {'train': training, 'val': validation, 'test': testing}\n",
    "        SplittedDataset['train'][fold].dataset (CerealTimeKillersDataset): Training dataset in nth fold\n",
    "        SplittedDataset['val'][fold].dataset (CerealTimeKillersDataset): Validation dataset in nth fold\n",
    "        SplittedDataset['test'].dataset (CerealTimeKillersDataset): Testing dataset outside folds\n",
    "    SplittedDataLength (dict): Length of dataset in {'train': training, 'val': validation, 'test': testing}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split into train/val and test datasets\n",
    "    train_set_index, test_set_index = list(), list()\n",
    "    if allocation_test == None:\n",
    "        test_size = int(test_ratio * len(full_dataset))\n",
    "        train_size = len(full_dataset) - test_size\n",
    "        train_set_orig, test_set_orig = random_split(full_dataset, \n",
    "                                                     [train_size, test_size], \n",
    "                                                     generator = generator)\n",
    "    elif (allocation_test == 'subject') or (allocation_test == 'game'):\n",
    "        train_set_index = exp_index[~exp_index[allocation_test].isin(target_test)].index.tolist()\n",
    "        test_set_index = exp_index[exp_index[allocation_test].isin(target_test)].index.tolist()\n",
    "        train_set_orig = Subset(full_dataset, train_set_index)\n",
    "        test_set_orig = Subset(full_dataset, test_set_index)\n",
    "    else:\n",
    "        print(\"Allocate testing dataset based on one of the 'Subject', 'Game', or None.\")\n",
    "        return None\n",
    "    \n",
    "    # Test dataset loader\n",
    "    test_loader = DataLoader(test_set_orig,\n",
    "                             batch_size = batch_size_test,\n",
    "                             num_workers = 0,\n",
    "                             generator = g_seed)\n",
    "    \n",
    "    # K-fold Cross Validator\n",
    "    train_loader, val_loader = [[]] * k_folds, [[]] * k_folds\n",
    "    kfold = KFold(n_splits = k_folds, shuffle = True, random_state = seed)\n",
    "    for fold, (train_i, val_i) in enumerate(kfold.split(train_set_orig)):\n",
    "        \n",
    "        # Sample train/test dataset from indices\n",
    "        train_sampler = SubsetRandomSampler(train_i, generator = g_seed)\n",
    "        val_sampler = SubsetRandomSampler(val_i, generator = g_seed)\n",
    "        \n",
    "        # Train/Validation dataset loader\n",
    "        train_loader[fold] = DataLoader(train_set_orig,\n",
    "                                        sampler = train_sampler,\n",
    "                                        batch_size = batch_size_train,\n",
    "                                        num_workers = 0,\n",
    "                                        generator = generator)\n",
    "        val_loader[fold] = DataLoader(train_set_orig,\n",
    "                                      sampler = val_sampler,\n",
    "                                      batch_size = batch_size_test,\n",
    "                                      num_workers = 0,\n",
    "                                      generator = generator)\n",
    "    \n",
    "    # return datasplitter\n",
    "    data_loader = {'train': train_loader, 'val': val_loader, 'test': test_loader}\n",
    "    data_length = {'train': len(train_sampler), 'val': len(val_sampler), 'test': len(test_set_orig)}\n",
    "    return data_loader, data_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6c16a",
   "metadata": {},
   "source": [
    "## Neural network setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b0a1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_loader, optimizer = None, criterion = nn.MSELoss()):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for (data, target) in train_loader:\n",
    "        data, target = data.type(torch.float).to(args['device']), target.type(torch.float).to(args['device'])\n",
    "        optimizer.zero_grad() \n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target) + args['l1'] * L1_norm(model) + args['l2'] * L2_norm(model)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "396918e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, test_loader, criterion = nn.MSELoss()):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0.0\n",
    "    acc = 0.0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in test_loader:\n",
    "            data, target = data.type(torch.float).to(args['device']), target.type(torch.float).to(args['device'])\n",
    "            output = model(data)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(output, 1)\n",
    "            _, labels = torch.max(target, 1)\n",
    "            acc += (predicted == labels).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "    return eval_loss / len(test_loader), acc * 100 / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2722c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(args, model, train_loader, val_loader, test_loader, \n",
    "                                    optimizer = None, criterion = nn.MSELoss()):\n",
    "    \n",
    "    model = model.to(args['device'])\n",
    "    \n",
    "    val_loss_list, train_loss_list, test_loss_list = [], [], []\n",
    "    val_acc_list, train_acc_list, test_acc_list = [], [], []\n",
    "    param_norm_list = []\n",
    "    for epoch in tqdm(range(args['epochs'])):\n",
    "        \n",
    "        train(args, model, train_loader, optimizer = optimizer, criterion = criterion)\n",
    "        param_norm = calculate_frobenius_norm(model)\n",
    "        \n",
    "        train_loss, train_acc = test(args, model, train_loader, criterion = criterion)\n",
    "        val_loss, val_acc = test(args, model, val_loader, criterion = criterion)\n",
    "        test_loss, test_acc = test(args, model, test_loader, criterion = criterion)\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "        val_acc_list.append(val_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        param_norm_list.append(param_norm)\n",
    "        \n",
    "        if ((epoch + 1) % 10 == 0) or (epoch + 1 == args['epochs']):\n",
    "            print('-----Epoch ', epoch + 1, '/', args['epochs'])\n",
    "            print('Train/Val/TEST MSE:', train_loss, val_loss, test_loss)\n",
    "            print('Train/Val/TEST Accuracy:', train_acc, val_acc, test_acc)\n",
    "\n",
    "    plot_loss_accuracy(train_loss_list, val_loss_list, test_loss_list, train_acc_list, val_acc_list, test_acc_list)\n",
    "\n",
    "    return (train_loss_list, val_loss_list, test_loss_list), (train_acc_list, val_acc_list, test_acc_list), param_norm_list, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6811dc37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main function of model simulation\n",
    "def CerealTimeKillersModelSimulator(args, TrainDataLoader, ValDataLoader, TestDataLoader, DataSize,\n",
    "                                    K_fold_train = False, k_folds = 1):\n",
    "    \n",
    "    N_FOLD = k_folds if K_fold_train else 1\n",
    "    loss, acc, param, models = [], [], [], []\n",
    "    \n",
    "    for fold in range(N_FOLD):\n",
    "        print('\\n%d/%d Fold' % (fold + 1, N_FOLD))\n",
    "        print('----------------------------')\n",
    "    \n",
    "        model, optimizer, criterion = CerealTimeKillersModelGenerator(args, size = DataSize)\n",
    "        loss_list, acc_list, param_norm_list, trained_model = simulation(args, model,\n",
    "                                                                         TrainDataLoader[fold],\n",
    "                                                                         ValDataLoader[fold],\n",
    "                                                                         TestDataLoader,\n",
    "                                                                         optimizer = optimizer,\n",
    "                                                                         criterion = criterion)\n",
    "    \n",
    "        loss_list, acc_list = np.array(loss_list), np.array(acc_list)\n",
    "        loss.append([loss_list[0, -1], loss_list[1, -1], loss_list[2, -1]])\n",
    "        acc.append([np.max(acc_list[0]), np.max(acc_list[1]), np.max(acc_list[2])])\n",
    "        param.append(param_norm_list[-1])\n",
    "        models.append(trained_model)\n",
    "    \n",
    "    print('Train/Val/Test Final MSE:', list(loss[-1]))\n",
    "    print('Train/Val/Test Maximum Accuracy:', list(acc[-1]))\n",
    "    \n",
    "    return loss, acc, param, models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fea67d",
   "metadata": {},
   "source": [
    "## Model settings - Change models here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c61e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTKNet(nn.Module):\n",
    "    def __init__(self, input_shape, out_size):\n",
    "\n",
    "        super(CTKNet, self).__init__()\n",
    "        \n",
    "        # Model hyperparametres (layer by layer)\n",
    "        conv_channel = [5, 5]\n",
    "        conv_kernel = [10, 3]\n",
    "        pool_kernel = [2, 2]\n",
    "        fc_unit = [128]\n",
    "        drop_out = [0.5]\n",
    "        \n",
    "        # Hidden layers\n",
    "        img_size = np.array(input_shape[1:])\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = input_shape[0], out_channels = conv_channel[0], kernel_size = conv_kernel[0])\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = pool_kernel[0])\n",
    "        img_size = np.floor((img_size - conv_kernel[0] + 1.0) / pool_kernel[0])\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = conv_channel[0], out_channels = conv_channel[1], kernel_size = conv_kernel[1])\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = pool_kernel[1])\n",
    "        img_size = np.floor((img_size - conv_kernel[1] + 1.0) / pool_kernel[1])\n",
    "        \n",
    "        fc_input_size = int(np.prod(img_size) * conv_channel[1])\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = fc_input_size, out_features = fc_unit[0])\n",
    "        self.drop1 = nn.Dropout(drop_out[0])\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features = fc_unit[0], out_features = out_size[0])\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ced14bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourNet(nn.Module):\n",
    "    def __init__(self, input_shape, out_size):\n",
    "\n",
    "        super(YourNet, self).__init__()\n",
    "        \n",
    "        # Model hyperparametres (layer by layer)\n",
    "        ...\n",
    "        \n",
    "        # Hidden layers\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "638894f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection function\n",
    "def CerealTimeKillersModelGenerator(args, size):\n",
    "    \n",
    "    model = CTKNet(input_shape = size[0], out_size = size[1])\n",
    "    optimizer = optim.SGD(model.parameters(), lr = args['lr'], momentum = args['momentum'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    return model, optimizer, criterion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6e261",
   "metadata": {},
   "source": [
    "## Input settings - Change hypermatres here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37a857d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CerealTimeKillersLabels:\n",
    "    \"\"\"\n",
    "    Select labels for model prediction\n",
    "    Labels used for prediction: info + electrode --> prediction\n",
    "    CHANGE these with necessity before loading data\n",
    "    \"\"\"\n",
    "    \n",
    "    # ['subject', 'game', 'gender', 'age', 'disturbance', 'experience', 'memory']\n",
    "    info = []\n",
    "        \n",
    "    # ['AF3', 'AF4', 'F3', 'F4', 'F7', 'F8', 'FC5', 'FC6', 'O1', 'O2', 'P7', 'P8', 'T7', 'T8']\n",
    "    electrode = ['AF3', 'AF4', 'F3', 'F4', 'F7', 'F8', 'FC5', 'FC6', 'O1', 'O2', 'P7', 'P8', 'T7', 'T8']\n",
    "        \n",
    "    # ['satisfied', 'boring', 'horrible', 'calm', 'funny', 'valence', 'arounsal']\n",
    "    prediction = ['boring', 'horrible', 'calm', 'funny']\n",
    "    # prediction = ['valence', 'arounsal']\n",
    "    \n",
    "    # Fixed variables\n",
    "    fixed = info + prediction\n",
    "    \n",
    "    # Summarise labels for model\n",
    "    label = info + electrode + prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20037d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -> `Change runtime type.`  select `GPU` \n",
      "Current device: cpu\n",
      "Random seed 2021 has been set.\n"
     ]
    }
   ],
   "source": [
    "# General settings\n",
    "workspace_dir = '' # Workspace directionary\n",
    "LabelRange = [0, 11] # The [min, max] of emotional states for transformation\n",
    "\n",
    "# Whether to allow between-window dataset mixture\n",
    "# SET TO FALSE FOR 4-DIMENSIONAL INPUT WHEN USING RNN\n",
    "Is_between_subject = True # Default is True for 3-dimensional input\n",
    "\n",
    "# Which to be based for allocating testing dataset (only when Is_between_subject = True)\n",
    "Allocation_test = None # [None, 'subject', 'game'] # Default is None\n",
    "test_ratio = 0.2 # Proportion of data used for testing when Allocation_test == None\n",
    "Target_test = [25, 26, 27] # Int list for allocating corresponding game/subject as testing dataset when Allocation_test != None\n",
    "\n",
    "# Model structural settings\n",
    "N_inputtime = None # Time window for input sampling (Default is None for the whole timepoints)\n",
    "N_stridetime = N_inputtime # Temporal leap for input sampling when N_inputtime != None\n",
    "N_perseg = 256 # N per seg of spectrogram\n",
    "N_framerate = 128 # Framerate of spectrogram\n",
    "\n",
    "# Model training settings\n",
    "batch_size_train = 16 # Number of examples per minibatch during training\n",
    "batch_size_test = 1 # Number of examples per minibatch during validation/testing\n",
    "k_folds = 5 # Number for K-folds for training vs validation (validation is 1/k_folds of the train/val set)\n",
    "K_fold_train = False # Whether enable the full K-fold cross-validation for training (if False, validate only once)\n",
    "\n",
    "# Model hypermparametres\n",
    "args = {\n",
    "    'epochs': 150,\n",
    "    'lr': 1e-4,\n",
    "    'momentum': 0.99,\n",
    "    'l1': 5e-5,\n",
    "    'l2': 5e-5,\n",
    "    'device': set_device(),\n",
    "}\n",
    "print('Current device:', args['device'])\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 2021\n",
    "set_seed(seed = SEED)\n",
    "g_seed = torch.Generator()\n",
    "g_seed.manual_seed(SEED)\n",
    "\n",
    "# Torch-based data transformation\n",
    "data_transform = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea47216",
   "metadata": {},
   "source": [
    "## MAIN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f1a3b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: {'train': 70, 'val': 17, 'test': 21}\n",
      "Input shape: [channel, frequency, time]\n",
      "Single input data size: (14, 129, 170)\n",
      "Single output data size: (4,)\n"
     ]
    }
   ],
   "source": [
    "# Implement Dataloader\n",
    "FullDataset, DataSize, ExpIndex = CerealTimeKillersDataLoader(dir_base = workspace_dir,\n",
    "                                                              label_class = CerealTimeKillersLabels,\n",
    "                                                              label_range = LabelRange,\n",
    "                                                              dataset_mix = Is_between_subject,\n",
    "                                                              winlen = N_inputtime,\n",
    "                                                              stride = N_stridetime,\n",
    "                                                              nperseg = N_perseg,\n",
    "                                                              fs = N_framerate,\n",
    "                                                              transform = data_transform)\n",
    "\n",
    "# Implement DataSplitter\n",
    "SplittedDataset, SplittedDataLength = CerealTimeKillersDataSplitter(FullDataset, \n",
    "                                                                    exp_index = ExpIndex, \n",
    "                                                                    allocation_test = Allocation_test,\n",
    "                                                                    test_ratio = test_ratio,\n",
    "                                                                    target_test = Target_test,\n",
    "                                                                    k_folds = k_folds,\n",
    "                                                                    batch_size_train = batch_size_train,\n",
    "                                                                    batch_size_test = batch_size_test,\n",
    "                                                                    seed = SEED,\n",
    "                                                                    generator = g_seed)\n",
    "\n",
    "# Load Splited data\n",
    "(TrainDataLoader, ValDataLoader, TestDataLoader) = (SplittedDataset['train'],\n",
    "                                                    SplittedDataset['val'],\n",
    "                                                    SplittedDataset['test'])\n",
    "\n",
    "# Show data size\n",
    "print('Dataset length:', SplittedDataLength)\n",
    "print('Input shape: [channel, frequency, time]')\n",
    "print('Single input data size:', DataSize[0])\n",
    "print('Single output data size:', DataSize[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec82d85c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTKNet(\n",
      "  (conv1): Conv2d(14, 5, kernel_size=(10, 10), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=5655, out_features=128, bias=True)\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 5, 120, 161]           7,005\n",
      "         MaxPool2d-2            [-1, 5, 60, 80]               0\n",
      "            Conv2d-3            [-1, 5, 58, 78]             230\n",
      "         MaxPool2d-4            [-1, 5, 29, 39]               0\n",
      "            Linear-5                  [-1, 128]         723,968\n",
      "           Dropout-6                  [-1, 128]               0\n",
      "            Linear-7                    [-1, 4]             516\n",
      "================================================================\n",
      "Total params: 731,719\n",
      "Trainable params: 731,719\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.17\n",
      "Forward/backward pass size (MB): 1.14\n",
      "Params size (MB): 2.79\n",
      "Estimated Total Size (MB): 5.10\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximilianeggl/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1623459044803/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "# Model selection\n",
    "model, optimizer, criterion = CerealTimeKillersModelGenerator(args, size = DataSize)\n",
    "print(model)\n",
    "summary(model, DataSize[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96f64e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1 Fold\n",
      "----------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c893936488d4e498eba153a7fb4bd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch  10 / 150\n",
      "Train/Val/TEST MSE: 0.05355195477604866 0.06503291883402401 0.10198474617763645\n",
      "Train/Val/TEST Accuracy: 50.72463768115942 38.888888888888886 23.80952380952381\n",
      "-----Epoch  20 / 150\n",
      "Train/Val/TEST MSE: 0.04720352366566658 0.05815195985552338 0.08196479219588496\n",
      "Train/Val/TEST Accuracy: 59.42028985507246 27.77777777777778 19.047619047619047\n"
     ]
    }
   ],
   "source": [
    "# Model simulation\n",
    "loss_K, acc_K, param_K, models_K = CerealTimeKillersModelSimulator(args,\n",
    "                                                                   TrainDataLoader,\n",
    "                                                                   ValDataLoader,\n",
    "                                                                   TestDataLoader,\n",
    "                                                                   DataSize,\n",
    "                                                                   K_fold_train = K_fold_train,\n",
    "                                                                   k_folds = k_folds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dee000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average results from K-folds\n",
    "print('Train/Val/Test Average MSE:', list(np.mean(np.array(loss_K), axis = 0)))\n",
    "print('Train/Val/Test Average Accuracy:', list(np.mean(np.array(acc_K), axis = 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954bae3a",
   "metadata": {},
   "source": [
    "## Single prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2341ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print single prediction results from data loader\n",
    "fold = 0\n",
    "ShowDataset = TestDataLoader\n",
    "ShowModel = models_K[fold]\n",
    "BATCH_SHOW = 20\n",
    "\n",
    "ShowModel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (data, target) in enumerate(ShowDataset):\n",
    "        data, target = data.type(torch.float).to(args['device']), target.type(torch.float).to(args['device'])\n",
    "        output = ShowModel(data)\n",
    "            \n",
    "        eval_loss = criterion(output, target).item()\n",
    "            \n",
    "        _, predicted = torch.max(output, 1)\n",
    "        _, labels = torch.max(target, 1)\n",
    "        eval_acc = (predicted == labels).sum().item() * 100.0 / target.size(0)\n",
    "        \n",
    "        if idx < BATCH_SHOW:\n",
    "            print('Batch', idx + 1, ' ( Size', target.size(0), '):')\n",
    "            print('Output Example:', output[0].detach().numpy(), 'with label ', predicted[0].detach().numpy())\n",
    "            print('Target Example:', target[0].detach().numpy(), 'with label ', labels[0].detach().numpy())\n",
    "            print('------- MSE:', eval_loss, ' Accuracy:', eval_acc, '%-------\\n')\n",
    "        else:\n",
    "            print('Etc. for totally ', len(ShowDataset), 'batches.')\n",
    "            break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc92555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
